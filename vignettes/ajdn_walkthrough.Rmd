---
title: "ajdn_walkthrough"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{ajdn_walkthrough}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(ajdn)
```

This package implements method from https://arxiv.org/abs/2410.23706. Any reference to equations/sections refers to those in version v2 submitted to arXiv on Saturday 23rd November 2024.

Here I will walk through a simple example of using AJDN on simulated data, and then one example on a real dataset.

# Simulated Data Example

First, we will simulate data using one of the same data generating processes that was presented in the paper. We will use the GS data generating process for $n=1000,\;p=10$. We add jumps based on Scenario 1 of the power simulations with $\gamma=0.2$; this means there is a jump in dimension 1 at $t=0.25$ and a jump in dimension 2 at $t=0.75$. The jumps are scaled to be of size 5 standard deviations of the error process.

We will first calculate the rule of thumb scales from appendix B to use for $\underline{s},\bar{s}$ (scales the same in every dimension), and then use `ajdn::sparsify_scales()` to generate a sparse sequence of scales between this range. Then we generate the error matrix and mean matrix (in this case the mean matrix contains the jumps). 

```{r simulate_setup,dpi=300,fig.width=6, fig.height=4, out.width="100%"}
set.seed(1)
n = 1000
p = 10
snr = 5 # signal to noise ratio of jump
dgp = 'GS'

rule_of_thumb_scales = scales_rot(n,p)
s = sparsify_scales(rule_of_thumb_scales[1],rule_of_thumb_scales[2],n,p) 
error_matrix = ajdn::generate_x_matrix(n,p,dgp,s)
mean_matrix = ajdn::add_changepoints_scaled(1,0.2,snr,error_matrix,s)
time_series_matrix = error_matrix + mean_matrix[[1]]

cat('Rule of thumb minimum and maximum scales selected ',s)
plot(time_series_matrix[1,],xlab='n x t',ylab='',main='Dimension 1 from Simulated GS Process \n with a Single Jump',type='l',cex.main=0.8)
```

Next, for AJDN we must choose the hyperparameter $s'$, the block size for the multiplier bootstrap. For the simulated data generating processes in the paper we can use the function `ajdn::s_prime_select()` in order to get the rule of thumb $s'$ based off of Equation 265. This function is related to using the autocovariance of the true data generating process to select $s'$.

```{r simulated_s_prime,dpi=300,fig.width=6, fig.height=4, out.width="100%"}
s_prime = ajdn::s_prime_select(dgp,n,p)
cat("s' using s_prime_select function ",s_prime)
```

Below is what would happen if we wished to use the sample autocovariance to try and select $s'$ based off of this rule of thumb. When using the rule of thumb to select $s'$, choose a representative dimension, and a region where there are no jumps in order to calculate the sample autocovariance. In this case I choose all datapoints from dimension 10 as there is no jump in this dimension. Then I calculate Equation 265 for a $s'=1,\dots,ns'_\text{max}$ and choose the $s'$ which makes this ratio as close to the rule of thumb provided in Appendix B.1.

```{r sample_autocov_s_prime_select,dpi=300,fig.width=6, fig.height=4, out.width="100%"}
# s_prime_max = ajdn::s_prime_max_rot(n)
# autocov_vec = as.numeric(acf(time_series_matrix[10,],type='covariance',plot=FALSE)$acf) 
# 
# LRV_ratio_numerator = autocov_vec[1] + 2*sum(autocov_vec[2:s_prime_max])
# LRV_ratio_denominators = bootstrap_lrv(seq(1,s_prime_max),autocov_vec)
# LRV_ratio = LRV_ratio_numerator/LRV_ratio_denominators
# LRV_ratio_rot = calc_desired_lrv_ratio(n)
# 
# s_prime_rot = seq(1,max_s_prime)[which.min(abs(LRV_ratio-LRV_ratio_rot))]/n
# cat("s' using sample autocovariance and rule of thumb =",s_prime_rot)
```

Now that the hyperparameters are selected, we can run AJDN

```{r find_cps,dpi=300,fig.width=6, fig.height=4, out.width="100%"}
set.seed(1)
B = 1000 # number of bootstraps to use
alpha = 0.05 # Type I error

cps_detected = ajdn::ajdn_detect_jumps(time_series_matrix,s,s_prime,B,alpha)

print('Jumps Detected')
print(cps_detected)
```

We also note that if using the value of $s'$ calculated using the sample autocovariance you end up with the same result.

```{r find_cps_other_s_prime,dpi=300,fig.width=6, fig.height=4, out.width="100%"}
# set.seed(1)
# B = 1000 # number of bootstraps to use
# alpha = 0.05 # Type I error
# 
# cps_detected = ajdn::ajdn_detect_jumps(time_series_matrix,s,s_prime_rot,B,alpha)
# 
# print('Jumps Detected when using s\' calculated with sample autocovariance')
# print(cps_detected)
```

Now we plot out two dimensions with a jump, and one without, to see where the jumps were detected. A practical note is that if a jump is detected at index $i$ then this implies the mean is significantly different between index $i$ and index $i+1$.

```{r cp_plots,dpi=300,fig.width=6, fig.height=4, out.width="100%"}
#par(mfrow=c(1,3))
for(dim in c(1,2,10)){
  par(mar=c(5,5,2,2))
  plot(time_series_matrix[dim,],xlab='n x t',ylab='',main=paste('Dim',dim),type='l')
  abline(v=cps_detected[cps_detected$dim==dim,'t_index'],col='red')
}
```

# Real Data Example - Prespecified Hyperparameters

Now we turn to a real data example. In the below code we are replicating the results from Appendix B.4 from the paper. This code shows how to access this dataset `ajdn::stocks_data` and then runs AJDN using the optimal $\underline{s},\bar{s},s'$ which we found.

```{r real_data_detect,dpi=300,fig.width=6, fig.height=4, out.width="100%"}
set.seed(1)
B = 1000
alpha = 0.05
s = sparsify_scales(.027,.044,length(ajdn::stocks_data),nrow(ajdn::stocks_data))
s_prime = 1/length(ajdn::stocks_data)

cps_detected=ajdn::ajdn_detect_jumps(ajdn::stocks_data,s,s_prime,B,alpha)

par(mar=c(5,5,2,2))
plot(y=as.numeric(ajdn::stocks_data['AMZN',]),x=as.Date(colnames(ajdn::stocks_data)),
     xlab='Date',ylab='AMZN/QQQ',main='AMZN jumps detected')
abline(v=as.Date(colnames(ajdn::stocks_data))[cps_detected[cps_detected$dim==which(rownames(stocks_data)=='AMZN'),'t_index']]
       ,col='red')
```

# Real Data Data-Driven Hyperparameter Select

One way to select the hyperparameters $\underline{s},\bar{s},s'$ is via a BIC-inspired data-driven method outlined in Appendix B.1.1 of the paper. Below we run through a simple example based on the stock dataset.

We use a rule of thumb to determine the max $s'$ to test ($s'_{\text{max}}$), and then based on assumption (B1) of the paper that jumps at least $2\bar{s}$ apart, we test a high scale of approximately 1/2 of a financial quarter (1/16 of the 2yr dataset we have).

Here in order to limit the number of combinations of hyperparameters to test, using the rule of thumb scales (from Appendix B) we calculate $\bar{s}^*/\underline{s}^*$ for the $n,p$ we are working with, and then for a given $\bar{s}$ test the value of $\underline{s}$ such that $\bar{s}/\underline{s}=\bar{s}^*/\underline{s}^*$. 

The range of scales to test is up to the discretion of the user. Testing $\underline{s}$ around $\underline{s}^*$, and similarly testing $\bar{s}$ around $\bar{s}^*$ is a reasonable approach. Alternatively if the user has any preexisting knoweldge of the data (e.g. how far jumps may be spaced apart) this could be useful to inform what scales to test. In our experience $s'$ is usually selected to be small, and the selection of $s'$ has a greater effect on the sensitivity of the method than the selection of $\underline{s},\bar{s}$.

```{r data_driven_hyperparameter,dpi=300,fig.width=6, fig.height=4, out.width="100%"}
set.seed(1)
B = 1000
alpha = 0.05

p = nrow(ajdn::stocks_data)
n = ncol(ajdn::stocks_data)

scale_to_test_number = 3

# Create a dataframe of hyperparameters to test
max_s_prime=round(n**(-2/3)*n)
rot_s= ajdn::scales_rot(n,p)
rot_ratio=rot_s[1]/rot_s[2]
rot_scale_range=rot_s[2]-rot_s[1]
potential_s_high=seq((1/16-rot_scale_range/2),1/16+rot_scale_range/2,length.out=scale_to_test_number)
bic_experiment_df=expand.grid(s_prime=seq(1,max_s_prime,by=2),s_high=potential_s_high)
bic_experiment_df[,c('bic','cps_detected')] = 0
```

Now for each combination of hyperparameters we wish to test, we will run AJDN, to detect jumps, and then based on these jumps calculate the penalized BIC decision criteria (section Appendix B.1.1 of paper). This BIC criteria can be accessed via the `ajdn::calc_bic` function

```{r data_driven_hyperparameter_fit,dpi=300,fig.width=6, fig.height=4, out.width="100%"}
for(i in 1:nrow(bic_experiment_df)){
  s_prime=bic_experiment_df[i,'s_prime']/n
  s_high=bic_experiment_df[i,'s_high']
  s_low=s_high*rot_ratio
  s = ajdn::sparsify_scales(s_low,s_high,n,p)

  cps_detected= ajdn::ajdn_detect_jumps(stocks_data,s,s_prime,B,alpha)

  bic_sum=0
  for(j in 1:p){
    bic_sum=bic_sum+ajdn::calc_bic(stocks_data[j,],
                                cps_detected[cps_detected$dim==j,],
                                s_prime)
  }
  bic_experiment_df[i,c('bic','cps_detected')] = c(bic_sum,nrow(cps_detected))
}

round(bic_experiment_df[order(bic_experiment_df$'bic'),],3)
```

Based on the above dataframe we see that $\bar{s}=0.043$ is optimal for the hyperparameters tested. This aligns closely with the value found in the paper (and result is shown in above section Real Data Example).
